---
name: fluxloop-prompt-compare
description: |
  Use for prompt version comparison and stability testing.
  Frequency: when tuning prompts. Optional â€” use when A/B comparison is needed.
  Keywords: compare, comparison, prompt version, stability, A/B test, diff, stability test, í”„ë¡¬í”„íŠ¸ ë¹„êµ

  Auto-activates on requests like:
  - "compare prompts", "compare prompt versions"
  - "stability test", "run a stability test"
  - "v3 vs v4", "version comparison"
  - "run same input multiple times", "run the same input multiple times"
  - "í”„ë¡¬í”„íŠ¸ ë¹„êµí•´ì¤˜", "ë¹„êµ í…ŒìŠ¤íŠ¸ ëŒë ¤ì¤˜"
---

# FluxLoop Prompt Compare Skill

**Same Bundle Ã— N Repeats Ã— Version Diff** â€” freeze inputs via bundle, automate repeated runs, compare outputs.

## Output Format

> ğŸ“ All user-facing output must follow: read skills/_shared/OUTPUT_FORMAT.md

## Context Protocol

1. `fluxloop context show` â†’ confirm project + scenario exist
2. `.fluxloop/test-memory/` check:
   - Exists â†’ load `agent-profile.md`, `results-log.md`
   - Missing â†’ proceed (first run)
3. Dual Write:
   - Server: `fluxloop test --scenario` (Ã—2 runs)
   - Local: save to `prompt-versions.md`, append to `results-log.md`
4. On completion: verify `prompt-versions.md` and `results-log.md` are current

> ğŸ“ Full protocol: read skills/_shared/CONTEXT_PROTOCOL.md
> ğŸ“ Stale detection: read skills/_shared/CONTEXT_COLLECTION.md

## Prerequisite

Run `fluxloop context show` first:
- âœ… Project + scenario exist â†’ proceed to Phase 0
- âŒ No project â†’ Prerequisite Resolution: setup ì¸ë¼ì¸ ì‹¤í–‰ ì œì•ˆ
- âŒ No scenario â†’ Prerequisite Resolution: scenario ì¸ë¼ì¸ ì‹¤í–‰ ì œì•ˆ
- Minimum: at least 1 bundle is needed (or will be created in Phase 1)

---

## Phase 0: Context Check

```bash
fluxloop context show
ls .fluxloop/scenarios
```

| State | Action |
|-------|--------|
| No scenario | â†’ "Start with 'ì‹œë‚˜ë¦¬ì˜¤ ë§Œë“¤ì–´ì¤˜' (scenario skill)" |
| Scenario exists | â†’ Phase 1 |

**test-memory read**:
1. Read `.fluxloop/test-memory/agent-profile.md`:
   - Extract `git_commit` from metadata â†’ compare with `git rev-parse --short HEAD`
   - Stale â†’ "í”„ë¡œí•„ì´ ì˜¤ë˜ëœ ê²ƒ ê°™ì€ë°, ì—…ë°ì´íŠ¸ í•´ë“œë¦´ê¹Œìš”?" â†’ Yes â†’ follow `_shared/CONTEXT_COLLECTION.md` inline
2. Read `.fluxloop/test-memory/results-log.md`:
   - If previous test records exist â†’ display as baseline reference

> ğŸ“ Stale detection: read skills/_shared/CONTEXT_COLLECTION.md

---

## Phase 1: Bundle Selection

> ğŸ“ Bundle selection: read skills/_shared/BUNDLE_DECISION.md (simplified flow for comparison tests)

```bash
fluxloop bundles list --scenario-id <scenario_id>
```

> **Tip:** For comparison tests, 1-3 inputs are usually enough. When creating new data, use `--total-count 2` for a small bundle.

Key info to display: **version/name, tag/description, input count, created date**

After bundle selected/created:

```bash
fluxloop sync pull --bundle-version-id <bundle_version_id>
```

> This bundle stays fixed throughout all comparison runs. Record the `bundle_version_id` for reuse.

---

## Phase 2: Comparison Setup

Ask the user:

> ğŸ’¡ **Repeats(ë°˜ë³µ íšŸìˆ˜)**: ë™ì¼ ì…ë ¥ì„ ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•˜ì—¬ ì‘ë‹µì˜ ì¼ê´€ì„±(stability)ì„ ì¸¡ì •í•©ë‹ˆë‹¤. ë°˜ë³µì´ ë§ì„ìˆ˜ë¡ í†µê³„ì ìœ¼ë¡œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë¹„êµê°€ ë©ë‹ˆë‹¤.

```
1. Number of repeats? (default: 5)
2. Multi-turn? (default: single-turn) â†’ if yes, also confirm max turns
3. Current prompt version label? (e.g., "v3", "current version")
```

Set iterations in `configs/simulation.yaml`:

```yaml
iterations: 5  # user-specified count
```

> Read existing simulation.yaml first. Only modify `iterations`, preserve all other fields.

> ğŸ“ Staging environment: read skills/_shared/STAGING.md

---

## Phase 3: Baseline Run (Version A)

### 3-1. Capture git diff (code snapshot)

```bash
git diff HEAD
```

Record the diff output â€” this captures the current code state before the run.

### 3-2. Run

```bash
# Single-turn (default)
fluxloop test --scenario <name>

# Multi-turn
! fluxloop test --scenario <name> --multi-turn --max-turns <N>
```

> âš ï¸ Multi-turn requires `!` prefix.
> ğŸ“ Multi-turn rules: read skills/_shared/MULTITURN.md

After completion:
1. Note experiment directory as `experiment_A`
2. **(Server)**: results stored automatically on server
3. **(Local)**: record Version A in `.fluxloop/test-memory/prompt-versions.md`:
   - Git ref, experiment ID, key characteristics
4. Output â€” **ë°˜ë“œì‹œ ğŸ”— ë§í¬ í¬í•¨**:
   `âœ… Baseline â†’ exp_<timestamp> (label: "v3", N runs) ğŸ”— https://alpha.app.fluxloop.ai/release/experiments/{experiment_id}/evaluation?project={project_id}`

---

## Phase 4: Prompt Modification

```
Please update the prompt.
Let me know when you're done, and share the new version label (e.g., "v4").
```

Wait for user confirmation. Do NOT modify any code yourself.

---

## Phase 5: Variant Run (Version B)

### 5-1. Capture git diff (what changed)

```bash
git diff HEAD
```

This shows exactly what the user changed between versions. Record for the comparison report.

### 5-2. Run

Same bundle, same inputs â€” only the prompt changed.

```bash
# Single-turn
fluxloop test --scenario <name>

# Multi-turn (same settings as baseline)
! fluxloop test --scenario <name> --multi-turn --max-turns <N>
```

> No need to `sync pull` again. The bundle is already pulled locally.
> ğŸ“ Multi-turn rules: read skills/_shared/MULTITURN.md

After completion:
1. Note experiment directory as `experiment_B`
2. **(Server)**: results stored automatically on server
3. **(Local)**: add Version B to `.fluxloop/test-memory/prompt-versions.md`:
   - Git ref, experiment ID, changes summary, git diff summary
4. **(Local)**: append comparison entry to `.fluxloop/test-memory/results-log.md`
5. Output â€” **ë°˜ë“œì‹œ ğŸ”— ë§í¬ í¬í•¨**:
   `âœ… Variant â†’ exp_<timestamp> (label: "v4", N runs) ğŸ”— https://alpha.app.fluxloop.ai/release/experiments/{experiment_id}/evaluation?project={project_id}`

---

## Phase 6: Comparison Analysis

### 6-1. Load Results

Read both experiment trace files:

```
.fluxloop/scenarios/<name>/experiments/<exp_A>/trace_summary.jsonl
.fluxloop/scenarios/<name>/experiments/<exp_B>/trace_summary.jsonl
```

> ğŸ“ Trace structure & analysis formats: read this file's references/analysis-metrics.md

### 6-2. Analyze & Report

Generate a comparison report with these sections:

#### 1) Prompt Changes (git diff summary)

```markdown
## Prompt Changes ({version_A} -> {version_B})
- [Summary of changed files and key edits]
```

#### 2) Per-Input Analysis

Group traces by `input` field, then compare across versions. Use the Per-Input Analysis Format from `references/analysis-metrics.md`.

#### 3) Overall Summary

Use the Overall Summary Table Format from `references/analysis-metrics.md`.

After analysis:
- **(Local)**: update comparison result in `.fluxloop/test-memory/results-log.md`:
  - Winner (A/B/tie), key difference summary
- **(Local)**: update `.fluxloop/test-memory/prompt-versions.md`:
  - Comparison Result section: winner, key difference

---

## Phase 7: Next Actions

```
Choose one:
1. Additional comparison â€” update prompt again and compare (-> Phase 4)
2. Server evaluation â€” run detailed analysis with `fluxloop evaluate`
3. Done
```

> ğŸ’¡ ì‹¤í—˜ URLì€ Phase 3, 5ì˜ ê²°ê³¼ ì¶œë ¥ì—ì„œ ì´ë¯¸ ì œê³µë©ë‹ˆë‹¤. ë‹¤ì‹œ í™•ì¸í•˜ë ¤ë©´ ìœ„ ì¶œë ¥ì„ ì°¸ì¡°í•˜ì„¸ìš”.

If "Additional comparison": loop back to Phase 4 (same bundle reused).
If "Server evaluation":
```bash
fluxloop evaluate --experiment-id <exp_B_id> --wait
```

---

## Error Handling

| Error | Response |
|-------|----------|
| No scenario exists | "Start with 'ì‹œë‚˜ë¦¬ì˜¤ ë§Œë“¤ì–´ì¤˜' (scenario skill)" |
| No bundle available | Guide to bundle creation (Phase 1) |
| Baseline run fails | Check wrapper setup, API key, network. Resolve before continuing. |
| Variant run fails | Same check. Do NOT compare partial results. |
| trace_summary.jsonl missing | Check experiment directory. Re-run if needed. |
| Different input counts between A/B | This should not happen (same bundle). Verify bundle_version_id. |
| Profile stale (git_commit mismatch) | Offer inline update via _shared/CONTEXT_COLLECTION.md |

## Next Steps

Comparison done. Available next actions:
- Loop back to Phase 4 for another comparison (same bundle reused)
- Deep analysis with server evaluation (evaluate skill)
- Run a full test with winning prompt (test skill)
- Refine scenario based on learnings (scenario skill)

## Quick Reference

| Step | Command |
|------|---------|
| Check | `fluxloop context show` |
| Bundle | `fluxloop bundles list --scenario-id <id>` |
| Pull | `fluxloop sync pull --bundle-version-id <id>` |
| Run | `fluxloop test --scenario <name>` |
| Evaluate | `fluxloop evaluate --experiment-id <id> --wait` |

> ğŸ“ Full CLI reference: read skills/_shared/QUICK_REFERENCE.md

## Key Rules

1. **Inputs come from Web** â€” generate via `inputs synthesize` or select from existing, never write base_inputs manually
2. **Small bundles for comparison** â€” recommend `--total-count 2` when generating new inputs for comparison
3. **Bundle = frozen inputs** â€” pull once, reuse across all comparison runs
4. **Never modify the user's prompt/agent code** â€” only the user does that
5. **Capture git diff before each run** â€” links code changes to result changes
6. **Only change `iterations` in simulation.yaml** â€” preserve all other config fields
7. **Always read trace_summary.jsonl** â€” it has per-run details needed for comparison
8. **Label experiments clearly** â€” use user-provided version labels throughout
9. **Multi-turn uses `!` prefix** â€” same as other skills
10. **Check `agent-profile.md` for staleness before starting** â€” update if git_commit mismatches
11. **Dual Write**: record versions to `prompt-versions.md` and results to `results-log.md` alongside server actions
12. **Use templates from `test-memory-template/`** for output format
