---
name: fluxloop-evaluate
description: |
  Use for evaluating test results, analyzing insights, and improving the agent.
  Frequency: after every test run. Core of the daily test â†’ evaluate â†’ fix loop.
  Keywords: evaluate, evaluation, improve, analyze results, insights, recommendations, re-test, í‰ê°€, ê°œì„ , ë¶„ì„

  Auto-activates on requests like:
  - "í‰ê°€í•´ì¤˜", "evaluate the results"
  - "ì—ì´ì „íŠ¸ ê°œì„ í•´ì¤˜", "improve my agent"
  - "ê²°ê³¼ ë¶„ì„í•´ì¤˜", "analyze results"
---

# FluxLoop Evaluate Skill

**Evaluate-Analyze-Improve**: Server evaluation â†’ Result analysis â†’ Insight recording â†’ Code improvement â†’ Re-test loop

## Output Format

> ğŸ“ All user-facing output must follow: read skills/_shared/OUTPUT_FORMAT.md

## Context Protocol

1. `fluxloop context show` â†’ confirm project / scenario / test results exist
2. `.fluxloop/test-memory/` check:
   - Exists â†’ load `agent-profile.md`, `results-log.md`, `test-strategy.md`
   - Missing â†’ "Run 'í…ŒìŠ¤íŠ¸ ëŒë ¤ì¤˜' first"
3. Dual Write:
   - Server: `fluxloop evaluate --experiment-id`
   - Local: save to `.fluxloop/test-memory/learnings.md`, append to `.fluxloop/test-memory/results-log.md`
4. On completion: verify `learnings.md` is current (next scenario skill reads it)

> ğŸ“ Full protocol: read skills/_shared/CONTEXT_PROTOCOL.md
> ğŸ“ Collection procedure: read skills/_shared/CONTEXT_COLLECTION.md

## Prerequisite

Run `fluxloop context show` first:
- âœ… Project selected + scenario exists + test completed â†’ proceed
- âŒ ëˆ„ë½ëœ ë‹¨ê³„ ê°ì§€ â†’ Prerequisite Resolution (ğŸ“ read skills/_shared/PREREQUISITE_RESOLUTION.md):
  - ëˆ„ë½ ë²”ìœ„ë¥¼ íŒŒì•…í•˜ê³  í•„ìš”í•œ ì²´ì¸ì„ ë‚˜ì—´í•œë‹¤:
    - setup ëˆ„ë½: "setup â†’ context â†’ scenario â†’ test ìˆœì„œë¡œ ì§„í–‰ì´ í•„ìš”í•©ë‹ˆë‹¤. ìˆœì„œëŒ€ë¡œ ì§„í–‰í• ê¹Œìš”?"
    - context ëˆ„ë½: "context â†’ scenario â†’ test ìˆœì„œë¡œ ì§„í–‰ì´ í•„ìš”í•©ë‹ˆë‹¤. ìˆœì„œëŒ€ë¡œ ì§„í–‰í• ê¹Œìš”?"
    - scenario ëˆ„ë½: "scenario â†’ test ìˆœì„œë¡œ ì§„í–‰ì´ í•„ìš”í•©ë‹ˆë‹¤. ìˆœì„œëŒ€ë¡œ ì§„í–‰í• ê¹Œìš”?"
    - testë§Œ ëˆ„ë½: "í…ŒìŠ¤íŠ¸ ì‹¤í–‰ì´ í•„ìš”í•©ë‹ˆë‹¤. ë¨¼ì € ì§„í–‰í• ê¹Œìš”?"
  - ìŠ¹ì¸ ì‹œ: í•„ìš”í•œ ìŠ¤í‚¬ì„ ìˆœì„œëŒ€ë¡œ ì¸ë¼ì¸ ì‹¤í–‰ â†’ ê° ì™„ë£Œ ì‹œ "âœ… {ìŠ¤í‚¬ëª…} ì™„ë£Œ." â†’ ëª¨ë‘ ì™„ë£Œ í›„ Step 1ë¡œ ë³µê·€
  - ê±°ë¶€ ì‹œ: ì¤‘ë‹¨

Verify test completion: check `.fluxloop/test-memory/results-log.md` has at least 1 entry, or run `fluxloop test results --scenario <name>`.

## Workflow

### Step 1: Context Load

- Read `.fluxloop/test-memory/agent-profile.md` â†’ stale check (compare `git_commit` vs `git rev-parse --short HEAD`)
  - Stale â†’ "í”„ë¡œí•„ì´ ì˜¤ë˜ëœ ê²ƒ ê°™ì€ë°, ì—…ë°ì´íŠ¸ í•´ë“œë¦´ê¹Œìš”?"
    - Yes â†’ run collection procedure inline (ğŸ“ read skills/_shared/CONTEXT_COLLECTION.md)
    - No â†’ continue with existing profile
- Read `.fluxloop/test-memory/results-log.md` â†’ identify latest experiment ID, pass/fail ratio, history
- Read `.fluxloop/test-memory/test-strategy.md` â†’ load Evaluation Criteria

### Step 2: Server Evaluation

Trigger server-side evaluation:

```bash
fluxloop evaluate --experiment-id <id> --wait
fluxloop evaluate --experiment-id <id> --wait --timeout 900 --poll-interval 5
```

- `--wait` polls until status is `completed`, `partial`, `failed`, or `cancelled`
- When finished as `completed` or `partial` with at least one run completed â†’ insights are generated
- If job stays `queued` >30s without `locked_at` â†’ warn: "Workers may be down or backlog is high"

**Dual Write**:
- (Server) Evaluation results stored on server
- (Local) Append "Evaluation Results" section to the matching experiment entry in `.fluxloop/test-memory/results-log.md`:

```markdown
### Evaluation Results (added after evaluate skill runs)

| Criterion | Score | Notes |
|-----------|-------|-------|
| {criterion name} | {score} | {brief comment} |

**Insight**: {key finding from this run}

**Server link**: ğŸ”— {experiment URL}
```

> **í•„ìˆ˜ ë§í¬ ì¶œë ¥**: ì•„ë˜ Web Handoff í˜•ì‹ì„ ë°˜ë“œì‹œ ë”°ë¥¸ë‹¤. CLI ì¶œë ¥ì—ì„œ `experiment_id`ì™€ `project_id`ë¥¼ ì¶”ì¶œí•˜ì—¬ URLì„ êµ¬ì„±í•œë‹¤.

**Web Handoff** â€” output after evaluation:

```
âœ… Evaluation â†’ N insights ğŸ”— https://alpha.app.fluxloop.ai/release/experiments/exp_abc/evaluation?project=proj_123
ğŸ“‹ Check detailed analysis in the web app:
  - Decision: gates, budgets, baseline comparison
  - Insights: findings by category (with severity)
  - Recommendations: improvement suggestions (with priority)
  - Baseline: set the current result as baseline
```

### Step 3: Result Analysis + Insight Recording

Sync and analyze:

```bash
fluxloop sync pull --bundle-version-id <id>
fluxloop test results --scenario <name>
```

Analyze: failure patterns, warning turns, contract violations, related code locations.

Save findings to `.fluxloop/test-memory/learnings.md`:
- Format: follow `test-memory-template/learnings.md`
- **Discovered Patterns**: add Strengths / Weaknesses with new evidence
- **Applied Improvements**: add new rows to the table
- **Warnings for Next Test**: issues requiring verification
- **Open Questions**: unanswered hypotheses
- If existing content â†’ update (preserve existing + add new evidence), never overwrite

This `learnings.md` is read by the scenario skill in the next cycle â†’ **core of the improvement loop**.

### Step 4: Code Fix Suggestions

Based on analysis, suggest agent code modifications with specific file:line references.

> **Code changes always require user confirmation**, even in Auto mode.

| Step | Interactive | Auto |
|------|------------|------|
| Result analysis | Show + confirm | Show only |
| Code fix | Suggest â†’ confirm (required) | Suggest â†’ confirm (required) |
| Re-test | Ask "Re-test?" | Auto-proceed |
| Re-evaluate | Ask "Re-evaluate?" | Auto-proceed |

After edits, the hook runs `fluxloop test --smoke --quiet` automatically.

### Step 5: Re-test Guidance

If code was modified â†’ guide to re-test:
- "í…ŒìŠ¤íŠ¸ ëŒë ¤ì¤˜" (test skill)
- Use the same bundle for comparison consistency:
  ```bash
  fluxloop sync pull --bundle-version-id <id>
  fluxloop test --scenario <name>
  ```

### Step 6: Re-evaluate (Iteration Loop)

If unsatisfied with re-test results â†’ repeat from Step 2:
- Run the evaluate workflow again for the same flow
- Each iteration appends to `learnings.md` Applied Improvements â†’ tracks improvement history

## Error Handling

| Error | Response |
|-------|----------|
| Experiment ID not found | `fluxloop test results --scenario <name>` to find recent experiment ID |
| Evaluation timeout | Retry with `--timeout 900 --poll-interval 5` |
| Evaluation status `failed` / `cancelled` | Possible server issue; check status in web app |
| `.fluxloop/test-memory/results-log.md` missing | Prerequisite Resolution ì ìš© â†’ test ì¸ë¼ì¸ ì‹¤í–‰ ì œì•ˆ |
| Worker delay (`queued` >30s without `locked_at`) | "Workers may be down or backlog is high" |

## Next Steps

Evaluation complete. Available next actions:
- Re-test with updated code (test skill)
- Compare prompt versions A vs B (prompt-compare skill)
- Improve scenario using learnings (scenario skill â€” reads learnings.md)

## Quick Reference

| Step | Command |
|------|---------|
| Check | `fluxloop context show` |
| Results | `fluxloop test results --scenario <name>` |
| Evaluate | `fluxloop evaluate --experiment-id <id> --wait` |
| Sync | `fluxloop sync pull --bundle-version-id <id>` |

> ğŸ“ Full CLI reference: read skills/_shared/QUICK_REFERENCE.md

## Key Rules

1. Always run `fluxloop context show` first â€” route to the correct step based on state
2. Stale check `agent-profile.md` on load (`git_commit` comparison)
3. Dual Write: server evaluation results + local `results-log.md` update simultaneously
4. Code changes always require user confirmation (Auto mode included â€” no exceptions)
5. `learnings.md`: update (preserve existing + add new evidence); `results-log.md`: append
6. Re-test / re-evaluate loop: repeat Steps 2-6 until satisfied
7. Cite specific file:line locations when suggesting code improvements
